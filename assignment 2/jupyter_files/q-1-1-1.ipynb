{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n",
    "\n",
    "The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n",
    "\n",
    "In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-validate Split\n",
    "\n",
    "   In this function, it splits the data into training data and test data and returns two dataframes train and validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_split(df,test_size):\n",
    "    if isinstance(test_size,float):\n",
    "        test_size = round(test_size*len(df))\n",
    "        \n",
    "    indices=df.index.tolist()\n",
    "    validate_indices = random.sample(population=indices,k=test_size)\n",
    "    \n",
    "    validate_df=df.loc[validate_indices]\n",
    "    train_df=df.drop(validate_indices)\n",
    "    return train_df,validate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculation distances\n",
    "This function calculates the distance of every feature with test feature. The distance can be euclidian manhatten or any other. This function takes one test point and one training point and returns distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances(train,test):\n",
    "    distance=0\n",
    "    for i in range(len(train)-1):\n",
    "        distance=distance+pow((train[i]-test[i]),2)\n",
    "#     print(distance)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min DIstance calculation\n",
    "This function calculates distance of test point with respect to every point and returns k points with minimum distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_k(training_data,test_sample,k):\n",
    "#     print(training_data[0],test_sample,k)\n",
    "    all_dist=dict()\n",
    "    for x in range(len(training_data)):\n",
    "#         print(x)\n",
    "        all_dist[x]=(distances(training_data[x],test_sample)) \n",
    "    \n",
    "    lists=(sorted(all_dist.items(), key = lambda kv:(kv[1], kv[0])))\n",
    "    k_least=[]\n",
    "    for i in range(k):\n",
    "        k_least.append(lists[i])\n",
    "    return k_least"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking (Prediction)\n",
    "This function predicts the category of test data by comparing it with k points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking(training_data,validation_data,k):\n",
    "    k_least=find_min_k(training_data,validation_data,k)\n",
    "#     print(k_least)\n",
    "    uniq=np.unique(training_data[:,-1])\n",
    "    uniq.sort()\n",
    "    types=[]\n",
    "    for i in range(len(uniq)):\n",
    "        types.append(0)\n",
    "    for x in k_least:\n",
    "        for i in range(len(uniq)):\n",
    "            if training_data[x[0],-1]==uniq[i]:\n",
    "                types[i]+=1\n",
    "            \n",
    "    indx=types.index(max(types))\n",
    "    predicted=uniq[indx]\n",
    "    return predicted\n",
    "#     print(k_least)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main ( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris(test_file):\n",
    "    df=pd.read_csv(\"Iris.csv\")\n",
    "    data=df.values\n",
    "    random.seed(0)\n",
    "    training_df,validation_df=train_validate_split(df,.2)\n",
    "    training_data=training_df.values\n",
    "    \n",
    "    validation_data=validation_df.values\n",
    "#     df_test=pd.read_csv(test_file)\n",
    "#     validation_data=df_test.values\n",
    "    \n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in validation_data:\n",
    "        predicted=checking(training_data,i,6)\n",
    "        if predicted==i[-1]:\n",
    "            if i[-1]==\"Iris-setosa\":\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if i[-1]==\"Iris-setosa\":\n",
    "                fp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "\n",
    "\n",
    "    accuracy1=(tp+tn)/(tp+tn+fn+fp)\n",
    "    if tp!=0:\n",
    "        precision1=tp/(tp+fp)\n",
    "        recall1=tp/(tp+fn)\n",
    "        f1_score1=2*(recall1 * precision1) / (recall1 + precision1)\n",
    "\n",
    "    else:\n",
    "        precision1=0\n",
    "        recall1=0\n",
    "        f1_score1=0\n",
    "    \n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in validation_data:\n",
    "        predicted=checking(training_data,i,6)\n",
    "        if predicted==i[-1]:\n",
    "            if i[-1]==\"Iris-virginica\":\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if i[-1]==\"Iris-virginica\":\n",
    "                fp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "\n",
    "\n",
    "    accuracy2=(tp+tn)/(tp+tn+fn+fp)\n",
    "    if tp!=0:\n",
    "        precision2=tp/(tp+fp)\n",
    "        recall2=tp/(tp+fn)\n",
    "        f1_score2=2*(recall2 * precision2) / (recall2 + precision2)\n",
    "\n",
    "    else:\n",
    "        precision2=0\n",
    "        recall2=0\n",
    "        f1_score2=0\n",
    "    \n",
    "        tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in validation_data:\n",
    "        predicted=checking(training_data,i,6)\n",
    "        if predicted==i[-1]:\n",
    "            if i[-1]==\"Iris-versicolor\":\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if i[-1]==\"Iris-versicolor\":\n",
    "                fp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "\n",
    "\n",
    "    accuracy3=(tp+tn)/(tp+tn+fn+fp)\n",
    "    if tp!=0:\n",
    "        precision3=tp/(tp+fp)\n",
    "        recall3=tp/(tp+fn)\n",
    "        f1_score3=2*(recall3 * precision3) / (recall3 + precision3)\n",
    "\n",
    "    else:\n",
    "        precision3=0\n",
    "        recall3=0\n",
    "        f1_score3=0\n",
    "    \n",
    "#     true=0\n",
    "#     for i in validation_data:\n",
    "#         predicted=checking(training_data,i,6)\n",
    "#         if predicted==i[-1]:\n",
    "#             true+=1\n",
    "#     total=len(validation_data)\n",
    "#     accuracy=true/total\n",
    "\n",
    "    print(\"Iris\")\n",
    "    print(\"accuracy :\",(accuracy1+accuracy2+accuracy3)/3)\n",
    "    print(\"precision :\",(precision1+precision2+precision3)/3)\n",
    "    print(\"recall :\",(recall1+recall2+recall3)/3)\n",
    "    print(\"f1_score :\",(f1_score1+f1_score2+f1_score3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robo1(test_file):\n",
    "    df=pd.read_csv(\"Robot1\",delimiter=\" \")\n",
    "    new_arr=df.values\n",
    "    random.seed(0)\n",
    "    training_df,validation_df=train_validate_split(df,.2)\n",
    "    training_data=training_df.values\n",
    "    \n",
    "    validation_data=validation_df.values\n",
    "#     df_test=pd.read_csv(test_file)\n",
    "#     validation_data=df_test.values\n",
    "    \n",
    "    training_data=training_data[:,1:len(training_data[0])]\n",
    "    validation_data=validation_data[:,1:len(validation_data[0])]\n",
    "    training_data[:,[-1,0]] = training_data[:,[0,-1]]\n",
    "    validation_data[:,[-1,0]] = validation_data[:,[0,-1]]\n",
    "    training_data=training_data[:,2:len(training_data[0])]\n",
    "    validation_data=validation_data[:,2:len(validation_data[0])]\n",
    "    \n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in validation_data:\n",
    "        predicted=checking(training_data,i,6)\n",
    "        if predicted==i[-1]:\n",
    "            if i[-1]==0:\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if i[-1]==1:\n",
    "                fp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "\n",
    "\n",
    "    accuracy=(tp+tn)/(tp+tn+fn+fp)\n",
    "    if tp!=0:\n",
    "        precision=tp/(tp+fp)\n",
    "        recall=tp/(tp+fn)\n",
    "        f1_score=2*(recall * precision) / (recall + precision)\n",
    "\n",
    "    else:\n",
    "        precision=0\n",
    "        recall=0\n",
    "        f1_score=0\n",
    "\n",
    "    print()\n",
    "    print(\"robot 1\")\n",
    "    print(\"accuracy :\",accuracy)\n",
    "    print(\"precision :\",precision)\n",
    "    print(\"recall :\",recall)\n",
    "    print(\"f1_score :\",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robo2(test_file):\n",
    "    df=pd.read_csv(\"Robot2\",delimiter=\" \")\n",
    "    new_arr=df.values\n",
    "    random.seed(0)\n",
    "    training_df,validation_df=train_validate_split(df,.2)\n",
    "    training_data=training_df.values\n",
    "    \n",
    "    validation_data=validation_df.values\n",
    "#     df_test=pd.read_csv(test_file)\n",
    "#     validation_data=df_test.values\n",
    "    \n",
    "    training_data=training_data[:,1:len(training_data[0])]\n",
    "    validation_data=validation_data[:,1:len(validation_data[0])]\n",
    "    training_data[:,[-1,0]] = training_data[:,[0,-1]]\n",
    "    validation_data[:,[-1,0]] = validation_data[:,[0,-1]]\n",
    "    training_data=training_data[:,2:len(training_data[0])]\n",
    "    validation_data=validation_data[:,2:len(validation_data[0])]\n",
    "    \n",
    "    tp=0\n",
    "    tn=0\n",
    "    fp=0\n",
    "    fn=0\n",
    "    for i in validation_data:\n",
    "        predicted=checking(training_data,i,6)\n",
    "        if predicted==i[-1]:\n",
    "            if i[-1]==0:\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if i[-1]==1:\n",
    "                fp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "\n",
    "\n",
    "    accuracy=(tp+tn)/(tp+tn+fn+fp)\n",
    "    if tp!=0:\n",
    "        precision=tp/(tp+fp)\n",
    "        recall=tp/(tp+fn)\n",
    "        f1_score=2*(recall * precision) / (recall + precision)\n",
    "\n",
    "    else:\n",
    "        precision=0\n",
    "        recall=0\n",
    "        f1_score=0\n",
    "\n",
    "    print()\n",
    "    print(\"robot 2\")\n",
    "    print(\"accuracy :\",accuracy)\n",
    "    print(\"precision :\",precision)\n",
    "    print(\"recall :\",recall)\n",
    "    print(\"f1_score :\",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    test_file=[]\n",
    "    iris(test_file)\n",
    "    robo1(test_file)\n",
    "    robo2(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris\n",
      "accuracy : 0.9665367121507472\n",
      "precision : 0.9824561403508771\n",
      "recall : 0.9351851851851851\n",
      "f1_score : 0.9568903942305477\n",
      "\n",
      "robot 1\n",
      "accuracy : 0.8\n",
      "precision : 0.75\n",
      "recall : 0.9230769230769231\n",
      "f1_score : 0.8275862068965517\n",
      "\n",
      "robot 2\n",
      "accuracy : 0.9166666666666666\n",
      "precision : 0.9090909090909091\n",
      "recall : 0.9090909090909091\n",
      "f1_score : 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
