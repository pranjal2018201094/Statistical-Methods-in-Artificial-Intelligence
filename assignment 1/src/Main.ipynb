{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.rename(columns={\"left\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>label</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>7</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.93</td>\n",
       "      <td>4</td>\n",
       "      <td>249</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>accounting</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.71</td>\n",
       "      <td>4</td>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.10             0.90               7                   286   \n",
       "1                0.89             0.93               4                   249   \n",
       "2                0.38             0.50               2                   132   \n",
       "3                0.95             0.71               4                   151   \n",
       "4                0.84             0.84               5                   163   \n",
       "\n",
       "   time_spend_company  Work_accident  label  promotion_last_5years  \\\n",
       "0                   4              0      1                      0   \n",
       "1                   3              0      0                      0   \n",
       "2                   3              0      1                      0   \n",
       "3                   4              0      0                      0   \n",
       "4                   3              0      0                      0   \n",
       "\n",
       "        sales  salary  \n",
       "0       sales     low  \n",
       "1       sales     low  \n",
       "2  accounting     low  \n",
       "3       sales  medium  \n",
       "4   technical     low  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "satisfaction_level       float64\n",
       "last_evaluation          float64\n",
       "number_project             int64\n",
       "average_montly_hours       int64\n",
       "time_spend_company         int64\n",
       "Work_accident              int64\n",
       "label                      int64\n",
       "promotion_last_5years      int64\n",
       "sales                     object\n",
       "salary                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11238 entries, 0 to 11237\n",
      "Data columns (total 10 columns):\n",
      "satisfaction_level       11238 non-null float64\n",
      "last_evaluation          11238 non-null float64\n",
      "number_project           11238 non-null int64\n",
      "average_montly_hours     11238 non-null int64\n",
      "time_spend_company       11238 non-null int64\n",
      "Work_accident            11238 non-null int64\n",
      "label                    11238 non-null int64\n",
      "promotion_last_5years    11238 non-null int64\n",
      "sales                    11238 non-null object\n",
      "salary                   11238 non-null object\n",
      "dtypes: float64(2), int64(6), object(2)\n",
      "memory usage: 878.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_split(df,test_size):\n",
    "    if isinstance(test_size,float):\n",
    "        test_size = round(test_size*len(df))\n",
    "        \n",
    "    indices=df.index.tolist()\n",
    "    validate_indices = random.sample(population=indices,k=test_size)\n",
    "    \n",
    "    validate_df=df.loc[validate_indices]\n",
    "    train_df=df.drop(validate_indices)\n",
    "    return train_df,validate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "train_df,validate_df=train_validate_split(df,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "validate_df.head()\n",
    "name_list=list(df)\n",
    "column_list=[1,2,3,4,5,7,8,9]\n",
    "# diction=dict()\n",
    "# diction[\"salary\"]=9\n",
    "# diction[\"sales\"]=8\n",
    "# diction[\"promotion_last_5years\"]=7\n",
    "# diction[\"Work_accident\"]=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check_Purity (Unique Values Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_purity(data):\n",
    "    unique_vals=np.unique(data[:,6])\n",
    "    if len(unique_vals)==1:\n",
    "        return unique_vals[0]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify (It will return data which is offen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    \n",
    "    label_column = data[:, 6]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_splits(data):\n",
    "    \n",
    "    potential_splits = {}\n",
    "    n_column=column_list\n",
    "    for column_index in range(n_columns):          # excluding the last column which is the label\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        unique_values.sort()\n",
    "        \n",
    "        type_of_feature = FEATURE_TYPES[column_index]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            potential_splits[column_index] = []\n",
    "            for index in range(len(unique_values)):\n",
    "                if index != 0:\n",
    "                    current_value = unique_values[index]\n",
    "                    previous_value = unique_values[index - 1]\n",
    "                    potential_split = (current_value + previous_value) / 2\n",
    "\n",
    "                    potential_splits[column_index].append(potential_split)\n",
    "        \n",
    "        # feature is categorical\n",
    "        # (there need to be at least 2 unique values, otherwise in the\n",
    "        # split_data function data_below would contain all data points\n",
    "        # and data_above would be empty)\n",
    "        elif len(unique_values) > 1:\n",
    "            potential_splits[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "    \n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    if type_of_feature == \"continuous\":\n",
    "        data_below = data[split_column_values <= split_value]\n",
    "        data_above = data[split_column_values >  split_value]\n",
    "    \n",
    "    # feature is categorical   \n",
    "    else:\n",
    "        data_below = data[split_column_values == split_value]\n",
    "        data_above = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_below, data_above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowest Overall Entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    \n",
    "    label_column = data[:, 6]\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n",
    "                      + p_data_above * calculate_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    overall_entropy = 9999\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "\n",
    "            if current_overall_entropy <= overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_cal(training_data,column_no):\n",
    "    unique_vals,unique_val_count=np.unique(training_data[:,column_no],return_counts=True)\n",
    "    total_len=len(training_data)\n",
    "    entropy_sum=0\n",
    "    dic=dict()\n",
    "    for j in range(len(unique_vals)):\n",
    "        true=0\n",
    "        false=0\n",
    "        for i in training_data:\n",
    "            if (i[6]==1)and(unique_vals[j]==i[column_no]):\n",
    "                true+=1\n",
    "            elif (i[6]==0)and(unique_vals[j]==i[column_no]):\n",
    "                false+=1\n",
    "\n",
    "        p_true=true/(true+false)\n",
    "        p_false=false/(true+false)\n",
    "        total=unique_val_count[j]\n",
    "#         print(total)\n",
    "        if p_true and p_false:\n",
    "            entropy=(-1)*(p_true*(math.log2(p_true))+p_false*(math.log2(p_false)))\n",
    "            \n",
    "            dic[unique_vals[j]]=entropy\n",
    "            \n",
    "#         print(entropy)\n",
    "            entropy_weighted=(entropy*total)/total_len\n",
    "            entropy_sum+=entropy_weighted\n",
    "#     print(entropy_sum)\n",
    "    return (entropy_sum,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_entropy(training_data):\n",
    "    true=0\n",
    "    false=0\n",
    "    for i in training_data[:,6]:\n",
    "        if i==1:\n",
    "            true=true+1\n",
    "        elif i==0:\n",
    "            false=false+1\n",
    "    p_true=true/(true+false)\n",
    "    p_false=false/(true+false)\n",
    "    entropy_of_training_data=(-1)*(p_true*(math.log2(p_true))+p_false*(math.log2(p_false)))\n",
    "    print(entropy_of_training_data)\n",
    "    \n",
    "    return entropy_of_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(training_data):\n",
    "    entropy_of_training_data=overall_entropy(training_data);\n",
    "    \n",
    "    entropy_of_unique_vals=dict()\n",
    "    entropies_list=dict()\n",
    "    for i in col_list:\n",
    "        temp,entropy_of_unique_vals[i]=entropy_cal(training_data,i)\n",
    "        entropies_list[i]=temp\n",
    "\n",
    "#     min\n",
    "#     for i in col_list:\n",
    "#     sorted_x = sorted(entropies_list.items(), key=operator.itemgetter(0))\n",
    "    sorted_x = sorted(entropies_list.items(), key=lambda x: x[1])\n",
    "    print()\n",
    "    selected=sorted_x[0][0]    \n",
    "    gain=sorted_x[0][1]\n",
    "\n",
    "#     info_gain=entropy_of_training_data-gain\n",
    "    print()\n",
    "    print(entropy_of_unique_vals)\n",
    "#     print(info_gain)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine_type_of_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_type_of_feature(df):\n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 15\n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(df, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    # data preparations\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = determine_type_of_feature(df)\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df           \n",
    "    \n",
    "    \n",
    "    # base cases\n",
    "    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify_data(data)\n",
    "        \n",
    "        return classification\n",
    "\n",
    "    \n",
    "    # recursive part\n",
    "    else:    \n",
    "        counter += 1\n",
    "\n",
    "        # helper functions \n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        # determine question\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            \n",
    "        # feature is categorical\n",
    "        else:\n",
    "            question = \"{} = {}\".format(feature_name, split_value)\n",
    "        \n",
    "        # instantiate sub-tree\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        # find answers (recursion)\n",
    "        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth)\n",
    "        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth)\n",
    "        \n",
    "        # If the answers are the same, then there is no point in asking the qestion.\n",
    "        # This could happen when the data is classified even though it is not pure\n",
    "        # yet (min_samples or max_depth base case).\n",
    "        if yes_answer == no_answer:\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tree = decision_tree_algorithm(train_df, max_depth=3)\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
